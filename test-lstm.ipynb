{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ea7a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 14:40:28.643918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.optimizers import Adam\n",
    "# outdated: from keras.preprocessing import sequence \n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import squarify\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07bd5878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 14:41:09.996542: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model_lstm = load_model('final_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc1965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(x):\n",
    "    data_length=[]\n",
    "    wnl=WordNetLemmatizer()\n",
    "    cleaned_text=[]\n",
    "    stop_words = set(stopwords.words('english')) # Load stop words\n",
    "    mbti_types = [ \"ISTJ\", \"ESFJ\", \"ISTJ\",\"ISFP\",\n",
    "                  \"ESTJ\", \"ESFP\", \"ENFP\",\"ISTP\",\n",
    "                  \"INFP\", \"ESTP\", \"INTP\", \"ENTP\", \n",
    "                  \"ENFJ\", \"INTJ\", \"ENTJ\", \"INFJ\" ]\n",
    "    mbti_types = [t.lower() for t in mbti_types]\n",
    "    \n",
    "    for sent in trump_tweets_df.Tweets:\n",
    "        \n",
    "        # Case Standardisation\n",
    "        sent=sent.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        # source: Gabriel Giraldo-Wingler https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python\n",
    "        sent=re.sub('(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*',' ',sent)\n",
    "        \n",
    "        # Remove punctuations\n",
    "        sent=re.sub('[^0-9a-z]',' ',sent)\n",
    "        \n",
    "        # Remove stop words\n",
    "        sent = \" \".join([word for word in sent.split() if word not in stop_words]) \n",
    "        \n",
    "        for t in mbti_types:\n",
    "            sent = re.sub(t, '', sent)\n",
    "            \n",
    "        # Lemmatize\n",
    "        sent = wnl.lemmatize(sent) \n",
    "        \n",
    "        data_length.append(len(sent.split())) #Split data, measure length of new filtered data\n",
    "        \n",
    "        cleaned_text.append(sent)\n",
    "        \n",
    "    return cleaned_text,data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ec4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in sentence.split() if len(word)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12512276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_trump_tweets(trump_tweets):\n",
    "    # Assuming trump_tweets is a list of tweets\n",
    "    cleaned_trump_tweets = []\n",
    "    \n",
    "    for tweet in trump_tweets:\n",
    "        # Apply the clean_data function to each tweet\n",
    "        cleaned_tweet, _ = clean_data([tweet])\n",
    "        cleaned_trump_tweets.append(cleaned_tweet[0])\n",
    "    \n",
    "    return cleaned_trump_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d45aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n",
    "# Define a function to preprocess the tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Tokenize the tweet\n",
    "    words = word_tokenize(tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join words back into a sentence\n",
    "    cleaned_tweet = ' '.join(words)\n",
    "    \n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e170fa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you to all of the television viewers tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you imagine if I had the small crowds that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NATO commander agrees members should pay up vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow, NATO's top commander just announced that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The \"Rust Belt\" was created by politicians lik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0  Thank you to all of the television viewers tha...\n",
       "1  Can you imagine if I had the small crowds that...\n",
       "2  NATO commander agrees members should pay up vi...\n",
       "3  Wow, NATO's top commander just announced that ...\n",
       "4  The \"Rust Belt\" was created by politicians lik..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweets_df = pd.read_csv('trumptweets.csv', encoding='ISO-8859-1')\n",
    "trump_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f360cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trump_tweets_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a02feea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Trump tweets using preprocess_trump_tweets function\n",
    "trump_tweets_df['cleaned_tweet'] = trump_tweets_df['Tweets'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db4463d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 21s 23ms/step\n",
      "Predicted MBTI Type for Trump's Tweets: ENFP\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "labelencoder = joblib.load('labelencoder_lstm.pkl')\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_len = 40  # Adjust to match the model's input length\n",
    "X_trump = tokenizer.texts_to_sequences(trump_tweets_df['cleaned_tweet'])\n",
    "X_trump = pad_sequences(X_trump, padding='post', maxlen=max_len)\n",
    "\n",
    "# Make a single prediction for all the tweets using the pre-trained model\n",
    "predictions = loaded_model_lstm.predict(X_trump)\n",
    "\n",
    "# Decode the prediction back to an MBTI type\n",
    "predicted_mbti_enc = np.argmax(predictions, axis=1)  # Get the index of the highest probability\n",
    "predicted_mbti = labelencoder.inverse_transform(predicted_mbti_enc)  # Inverse transform to get the MBTI label\n",
    "\n",
    "# Print the predicted MBTI type\n",
    "print(\"Predicted MBTI Type for Trump's Tweets:\", predicted_mbti[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed675b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
