{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c43de2",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c659e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suhyun/opt/anaconda3/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:15: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.optimizers import Adam\n",
    "# outdated: from keras.preprocessing import sequence \n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import squarify\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688545cd",
   "metadata": {},
   "source": [
    "# Load Final GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5854bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('final_gru_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a4bd7",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1978856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to clean tweets \n",
    "def clean_data(x):\n",
    "    data_length=[]\n",
    "    wnl=WordNetLemmatizer()\n",
    "    cleaned_text=[]\n",
    "    stop_words = set(stopwords.words('english')) # Load stop words\n",
    "    mbti_types = [ \"ISTJ\", \"ESFJ\", \"ISTJ\",\"ISFP\",\n",
    "                  \"ESTJ\", \"ESFP\", \"ENFP\",\"ISTP\",\n",
    "                  \"INFP\", \"ESTP\", \"INTP\", \"ENTP\", \n",
    "                  \"ENFJ\", \"INTJ\", \"ENTJ\", \"INFJ\" ]\n",
    "    mbti_types = [t.lower() for t in mbti_types]\n",
    "    \n",
    "    for sent in trump_tweets_df.Tweets:\n",
    "        \n",
    "        # Case Standardisation\n",
    "        sent=sent.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        # source: Gabriel Giraldo-Wingler https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python\n",
    "        sent=re.sub('(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*',' ',sent)\n",
    "        \n",
    "        # Remove punctuations\n",
    "        sent=re.sub('[^0-9a-z]',' ',sent)\n",
    "        \n",
    "        # Remove stop words\n",
    "        sent = \" \".join([word for word in sent.split() if word not in stop_words]) \n",
    "        \n",
    "        for t in mbti_types:\n",
    "            sent = re.sub(t, '', sent)\n",
    "            \n",
    "        # Lemmatize\n",
    "        sent = wnl.lemmatize(sent) \n",
    "        \n",
    "        data_length.append(len(sent.split())) #Split data, measure length of new filtered data\n",
    "        \n",
    "        cleaned_text.append(sent)\n",
    "        \n",
    "    return cleaned_text,data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2357cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in sentence.split() if len(word)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9ca1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_trump_tweets(trump_tweets):\n",
    "    # Assuming trump_tweets is a list of tweets\n",
    "    cleaned_trump_tweets = []\n",
    "    \n",
    "    for tweet in trump_tweets:\n",
    "        # Apply the clean_data function to each tweet\n",
    "        cleaned_tweet, _ = clean_data([tweet])\n",
    "        cleaned_trump_tweets.append(cleaned_tweet[0])\n",
    "    \n",
    "    return cleaned_trump_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f883446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Define a function to preprocess the tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Tokenize the tweet\n",
    "    words = word_tokenize(tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join words back into a sentence\n",
    "    cleaned_tweet = ' '.join(words)\n",
    "    \n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd548aa",
   "metadata": {},
   "source": [
    "# Load Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "948ed604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you to all of the television viewers tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you imagine if I had the small crowds that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NATO commander agrees members should pay up vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow, NATO's top commander just announced that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The \"Rust Belt\" was created by politicians lik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0  Thank you to all of the television viewers tha...\n",
       "1  Can you imagine if I had the small crowds that...\n",
       "2  NATO commander agrees members should pay up vi...\n",
       "3  Wow, NATO's top commander just announced that ...\n",
       "4  The \"Rust Belt\" was created by politicians lik..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweets_df = pd.read_csv('trumptweets.csv', encoding='ISO-8859-1')\n",
    "trump_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa9eeec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trump_tweets_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7cfcbd",
   "metadata": {},
   "source": [
    "# Clean Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d310c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Trump tweets using preprocess_tweet function\n",
    "trump_tweets_df['cleaned_tweet'] = trump_tweets_df['Tweets'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7cbd45",
   "metadata": {},
   "source": [
    "# Preprocess tweets and predict MBTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8155f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 10s 11ms/step\n",
      "Predicted MBTI Type for Trump's Tweets: INFP\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "labelencoder = joblib.load('labelencoder.pkl')\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_len = 40  # Adjust to match the model's input length\n",
    "X_trump = tokenizer.texts_to_sequences(trump_tweets_df['cleaned_tweet'])\n",
    "X_trump = pad_sequences(X_trump, padding='post', maxlen=max_len)\n",
    "\n",
    "# Make a single prediction for all the tweets using the pre-trained model\n",
    "predictions = loaded_model.predict(X_trump)\n",
    "\n",
    "# Decode the prediction back to an MBTI type\n",
    "predicted_mbti_enc = np.argmax(predictions, axis=1)  # Get the index of the highest probability\n",
    "predicted_mbti = labelencoder.inverse_transform(predicted_mbti_enc)  # Inverse transform to get the MBTI label\n",
    "\n",
    "# Print the predicted MBTI type\n",
    "print(\"Predicted MBTI Type for Trump's Tweets:\", predicted_mbti[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
