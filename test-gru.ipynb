{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92ea7a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suhyun/opt/anaconda3/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:15: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.optimizers import Adam\n",
    "# outdated: from keras.preprocessing import sequence \n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import squarify\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07bd5878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('final_gru_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fc1965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(x):\n",
    "    data_length=[]\n",
    "    wnl=WordNetLemmatizer()\n",
    "    cleaned_text=[]\n",
    "    stop_words = set(stopwords.words('english')) # Load stop words\n",
    "    mbti_types = [ \"ISTJ\", \"ESFJ\", \"ISTJ\",\"ISFP\",\n",
    "                  \"ESTJ\", \"ESFP\", \"ENFP\",\"ISTP\",\n",
    "                  \"INFP\", \"ESTP\", \"INTP\", \"ENTP\", \n",
    "                  \"ENFJ\", \"INTJ\", \"ENTJ\", \"INFJ\" ]\n",
    "    mbti_types = [t.lower() for t in mbti_types]\n",
    "    \n",
    "    for sent in trump_tweets_df.Tweets:\n",
    "        \n",
    "        # Case Standardisation\n",
    "        sent=sent.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        # source: Gabriel Giraldo-Wingler https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python\n",
    "        sent=re.sub('(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*',' ',sent)\n",
    "        \n",
    "        # Remove punctuations\n",
    "        sent=re.sub('[^0-9a-z]',' ',sent)\n",
    "        \n",
    "        # Remove stop words\n",
    "        sent = \" \".join([word for word in sent.split() if word not in stop_words]) \n",
    "        \n",
    "        for t in mbti_types:\n",
    "            sent = re.sub(t, '', sent)\n",
    "            \n",
    "        # Lemmatize\n",
    "        sent = wnl.lemmatize(sent) \n",
    "        \n",
    "        data_length.append(len(sent.split())) #Split data, measure length of new filtered data\n",
    "        \n",
    "        cleaned_text.append(sent)\n",
    "        \n",
    "    return cleaned_text,data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ec4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in sentence.split() if len(word)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec61105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean2 = df2\n",
    "# df_clean2.posts, df_clean2_length = clean_data(df2)\n",
    "# df_clean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data=df_clean2['Tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12512276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_trump_tweets(trump_tweets):\n",
    "    # Assuming trump_tweets is a list of tweets\n",
    "    cleaned_trump_tweets = []\n",
    "    \n",
    "    for tweet in trump_tweets:\n",
    "        # Apply the clean_data function to each tweet\n",
    "        cleaned_tweet, _ = clean_data([tweet])\n",
    "        cleaned_trump_tweets.append(cleaned_tweet[0])\n",
    "    \n",
    "    return cleaned_trump_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d45aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n",
    "# Define a function to preprocess the tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Tokenize the tweet\n",
    "    words = word_tokenize(tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join words back into a sentence\n",
    "    cleaned_tweet = ' '.join(words)\n",
    "    \n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e170fa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you to all of the television viewers tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you imagine if I had the small crowds that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NATO commander agrees members should pay up vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow, NATO's top commander just announced that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The \"Rust Belt\" was created by politicians lik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0  Thank you to all of the television viewers tha...\n",
       "1  Can you imagine if I had the small crowds that...\n",
       "2  NATO commander agrees members should pay up vi...\n",
       "3  Wow, NATO's top commander just announced that ...\n",
       "4  The \"Rust Belt\" was created by politicians lik..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweets_df = pd.read_csv('trumptweets.csv', encoding='ISO-8859-1')\n",
    "trump_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "596e2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tweet text column from the DataFrame\n",
    "trump_tweets = trump_tweets_df['Tweets'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f360cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trump_tweets_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a02feea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trump_tweets['cleaned_tweet'] = trump_tweets['Tweet'].apply(preprocess_tweet)\n",
    "# Preprocess Trump tweets using preprocess_trump_tweets function\n",
    "#cleaned_trump_tweets = preprocess_trump_tweets(trump_tweets)\n",
    "trump_tweets_df['cleaned_tweet'] = trump_tweets_df['Tweets'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0793cdd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_trump_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tl/pjvv24r97gb_70lb57b6jp680000gn/T/ipykernel_92727/3864703525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Tokenize and pad the preprocessed Trump tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_trump_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtrump_tweets_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_trump_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrump_tweets_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrump_tweets_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_trump_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "max_words=2500\n",
    "\n",
    "# Tokenize and pad the preprocessed Trump tweets\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(cleaned_trump_tweets)\n",
    "trump_tweets_data = tokenizer.texts_to_sequences(cleaned_trump_tweets)\n",
    "trump_tweets_data = pad_sequences(trump_tweets_data, maxlen=max_len, padding='post')\n",
    "\n",
    "# Predict MBTI types for Trump tweets\n",
    "predictions = loaded_model.predict(trump_tweets_data)\n",
    "\n",
    "# Print or process the predictions as needed\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd1e2537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 10s 11ms/step\n",
      "Predicted MBTI Type for Trump's Tweets: INFP\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "labelencoder = joblib.load('labelencoder.pkl')\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_len = 40  # Adjust to match the model's input length\n",
    "X_trump = tokenizer.texts_to_sequences(trump_tweets_df['cleaned_tweet'])\n",
    "X_trump = pad_sequences(X_trump, padding='post', maxlen=max_len)\n",
    "\n",
    "# Make a single prediction for all the tweets using the pre-trained model\n",
    "predictions = loaded_model.predict(X_trump)\n",
    "\n",
    "# Decode the prediction back to an MBTI type\n",
    "predicted_mbti_enc = np.argmax(predictions, axis=1)  # Get the index of the highest probability\n",
    "predicted_mbti = labelencoder.inverse_transform(predicted_mbti_enc)  # Inverse transform to get the MBTI label\n",
    "\n",
    "# Print the predicted MBTI type\n",
    "print(\"Predicted MBTI Type for Trump's Tweets:\", predicted_mbti[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4463d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed675b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
